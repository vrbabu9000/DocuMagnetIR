{"pages":[{"image_id":"2025_04_27_83fd0f2a4bf8c26df6feg-1","page":1,"lines":[{"text":"\\title{\nCS $276$ / LING $286$ Information Retrieval and Web Search","cnt":[[350,308],[350,245],[1781,245],[1781,308]],"region":{"top_left_x":349,"top_left_y":245,"width":1432,"height":63},"line":1,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.9985356330871582,"confidence_rate":0.9999774550211274},{"text":" Problem Set $1$\n}","cnt":[[812,416],[812,343],[1302,343],[1302,416]],"region":{"top_left_x":811,"top_left_y":342,"width":492,"height":75},"line":2,"column":0,"font_size":56,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\n\\author{\nDue: Tuesday, April 30, $2019$ at 16:00 PDT\n}","cnt":[[603,530],[603,468],[1522,468],[1522,530]],"region":{"top_left_x":603,"top_left_y":467,"width":919,"height":63},"line":3,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\n\\section{General Instructions}","cnt":[[812,717],[812,660],[1313,660],[1313,717]],"region":{"top_left_x":811,"top_left_y":659,"width":503,"height":58},"line":4,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\nNote: Problem 3d has been updated as of Friday, April $26$ at $5$ pm PT. The previous version of","cnt":[[243,820],[243,774],[1882,774],[1882,820]],"region":{"top_left_x":242,"top_left_y":773,"width":1641,"height":47},"line":5,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.6570727009093389,"confidence_rate":0.9959310133681615},{"text":" the question read \"...For the given postings list, what is the upper limit on how much processing","cnt":[[244,862],[244,819],[1881,819],[1881,862]],"region":{"top_left_x":243,"top_left_y":818,"width":1639,"height":44},"line":6,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" time (in $\\mu$ s per list element) variable byte encoding can take, in order to be faster than (or","cnt":[[244,905],[244,862],[1881,862],[1881,905]],"region":{"top_left_x":243,"top_left_y":861,"width":1639,"height":45},"line":7,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.96044921875,"confidence_rate":0.9996044491550833},{"text":" equally fast as) gamma encoding?\" The current version reads \"For the given postings list, what","cnt":[[244,945],[244,899],[1881,899],[1881,945]],"region":{"top_left_x":243,"top_left_y":898,"width":1639,"height":47},"line":8,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" is the upper limit on how much processing time the decoding of variable byte encoding can","cnt":[[244,987],[244,943],[1881,943],[1881,987]],"region":{"top_left_x":243,"top_left_y":943,"width":1639,"height":44},"line":9,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" take (in $\\mu$ s per list element), in order to be faster than (or equally fast as) gamma encoding","cnt":[[244,1027],[244,984],[1881,984],[1881,1027]],"region":{"top_left_x":243,"top_left_y":983,"width":1639,"height":45},"line":10,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.7801055908203125,"confidence_rate":0.9975683925741097},{"text":" on an overall basis (transfer + decoding)?\"","cnt":[[243,1069],[243,1023],[1004,1023],[1004,1069]],"region":{"top_left_x":242,"top_left_y":1023,"width":763,"height":47},"line":11,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9853832484279792,"confidence_rate":0.9996932839472926},{"text":"\n\nThis problem set has $5$ problems. For some questions, you only need to write your answer as a number or a","cnt":[[244,1128],[244,1084],[1881,1084],[1881,1128]],"region":{"top_left_x":243,"top_left_y":1084,"width":1639,"height":44},"line":12,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" few characters; but you need to write a brief justification for your answer if the question requests you to do","cnt":[[244,1168],[244,1125],[1881,1125],[1881,1168]],"region":{"top_left_x":243,"top_left_y":1124,"width":1639,"height":45},"line":13,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9990236759185791,"confidence_rate":0.999991506114367},{"text":" so. Please make sure that your answer is clear and succinct; we may not give full credits to answers that","cnt":[[244,1210],[244,1164],[1881,1164],[1881,1210]],"region":{"top_left_x":243,"top_left_y":1164,"width":1639,"height":47},"line":14,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" are overly long. After you finish this problem set, upload the PDF file with your answers to GradeScope.","cnt":[[243,1252],[243,1209],[1839,1209],[1839,1252]],"region":{"top_left_x":242,"top_left_y":1208,"width":1598,"height":45},"line":15,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9965834617614746,"confidence_rate":0.9999686024384835},{"text":"\n\nIt is your responsibility to ensure that you finish this problem set individually. You can have general","cnt":[[244,1313],[244,1267],[1881,1267],[1881,1313]],"region":{"top_left_x":243,"top_left_y":1267,"width":1639,"height":47},"line":16,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" knowledge-level discussions with other students, but sharing answers is not allowed under the Honor Code.","cnt":[[244,1362],[244,1316],[1881,1316],[1881,1362]],"region":{"top_left_x":243,"top_left_y":1315,"width":1639,"height":48},"line":17,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" If you have questions or need clarifications, please post on Piazza or come to office hours.","cnt":[[244,1412],[244,1363],[1610,1363],[1610,1412]],"region":{"top_left_x":243,"top_left_y":1363,"width":1368,"height":50},"line":18,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\n\\section{Problem $1$ - Positional Indexes (5 points)}","cnt":[[243,1552],[243,1484],[1243,1484],[1243,1552]],"region":{"top_left_x":242,"top_left_y":1483,"width":1001,"height":69},"line":19,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.8111412835820602,"confidence_rate":0.995737416200732},{"text":"\n\nConsider the following documents:","cnt":[[243,1652],[243,1601],[820,1601],[820,1652]],"region":{"top_left_x":242,"top_left_y":1600,"width":578,"height":53},"line":20,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nDoc 1: I am a student, and I currently take CS276. I was a student in CS224N last quarter.","cnt":[[244,1710],[244,1662],[1773,1662],[1773,1710]],"region":{"top_left_x":243,"top_left_y":1661,"width":1530,"height":50},"line":21,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999948589553403},{"text":"\nDoc 2: I was a student. I have taken CS276.","cnt":[[245,1773],[245,1724],[991,1724],[991,1773]],"region":{"top_left_x":245,"top_left_y":1723,"width":746,"height":50},"line":22,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.999989825041388},{"text":"\na. We have seen that positional indexes are very useful to run queries and search against documents.","cnt":[[245,1877],[245,1831],[1880,1831],[1880,1877]],"region":{"top_left_x":245,"top_left_y":1830,"width":1635,"height":47},"line":23,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\nLet's build positional indexes on top of these documents using the format $\\mathrm{DocID}_{\\mathrm{a}}$ : $<$ Position $_{\\mathrm{abc}}$,","cnt":[[291,1927],[291,1878],[1877,1878],[1877,1927]],"region":{"top_left_x":291,"top_left_y":1878,"width":1586,"height":50},"line":24,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.33975851379329175,"confidence_rate":0.99209377165298},{"text":"\nPosition $_{\\mathrm{xyz}}>$; DocID $_{\\mathrm{b}}$ : $<$ Position $_{\\text {def }}$, Position $_{\\text {ghi }}>; \\ldots$","cnt":[[291,1977],[291,1926],[1172,1926],[1172,1977]],"region":{"top_left_x":291,"top_left_y":1925,"width":882,"height":53},"line":25,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.03311877774108518,"confidence_rate":0.9722304279640368},{"text":"\nThe position of the first word in the document is $1$, by convention.","cnt":[[291,2018],[291,1972],[1400,1972],[1400,2018]],"region":{"top_left_x":291,"top_left_y":1971,"width":1109,"height":47},"line":26,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9921910762786865,"confidence_rate":0.9998968530433137},{"text":"\nFor example, the positional index for the word \"am\" is as follows:","cnt":[[291,2086],[291,2034],[1389,2034],[1389,2086]],"region":{"top_left_x":291,"top_left_y":2034,"width":1099,"height":52},"line":27,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nam: $1:<2>$","cnt":[[373,2159],[373,2113],[592,2113],[592,2159]],"region":{"top_left_x":372,"top_left_y":2112,"width":221,"height":47},"line":28,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9297111762308783,"confidence_rate":0.9944094315425244},{"text":"\nShow the positional indexes for the words \"I\" and \"CS276\".","cnt":[[291,2243],[291,2191],[1292,2191],[1292,2243]],"region":{"top_left_x":291,"top_left_y":2191,"width":1001,"height":52},"line":29,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999922476411908},{"text":"\nb. A phrase query \"word1 word2\" retrieves the occurrences where word1 is immediately followed by","cnt":[[243,2319],[243,2267],[1882,2267],[1882,2319]],"region":{"top_left_x":242,"top_left_y":2267,"width":1641,"height":52},"line":30,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9931745482608676,"confidence_rate":0.9999321918864762},{"text":" word2. A / $k$ query \"word1 $/ k$ word2\" ( $k$ is a positive integer) retrieves the occurrences of word1","cnt":[[289,2368],[289,2316],[1880,2316],[1880,2368]],"region":{"top_left_x":288,"top_left_y":2316,"width":1592,"height":52},"line":31,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.8833425957496994,"confidence_rate":0.9988831272035097}],"page_height":2750,"page_width":2125,"languages_detected":[]},{"image_id":"2025_04_27_83fd0f2a4bf8c26df6feg-2","page":2,"lines":[{"text":"\nwithin $k$ words of word2 on either side. For example, $k=1$ demands that word1 be adjacent to","cnt":[[293,300],[293,253],[1881,253],[1881,300]],"region":{"top_left_x":292,"top_left_y":253,"width":1590,"height":47},"line":1,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9570050239562988,"confidence_rate":0.9995649794226655},{"text":" word2, but word1 may come either before or after word2.","cnt":[[291,348],[291,302],[1248,302],[1248,348]],"region":{"top_left_x":291,"top_left_y":302,"width":958,"height":47},"line":2,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.998046875,"confidence_rate":0.9999674166169199},{"text":"\n\nFor the following queries, return all the docs and corresponding positions for which the query","cnt":[[291,412],[291,363],[1877,363],[1877,412]],"region":{"top_left_x":291,"top_left_y":363,"width":1586,"height":49},"line":3,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" conditions are met. To take a hypothetical example: if a two-word query matches words $3$ and","cnt":[[291,457],[291,411],[1877,411],[1877,457]],"region":{"top_left_x":291,"top_left_y":410,"width":1586,"height":47},"line":4,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9990234375,"confidence_rate":0.9999901309613475},{"text":" $4$, as well as words $7$ and $8$ in document $2$, you should return the following:","cnt":[[291,507],[291,458],[1546,458],[1546,507]],"region":{"top_left_x":291,"top_left_y":458,"width":1256,"height":49},"line":5,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.934380054473877,"confidence_rate":0.99924615080999},{"text":"\n$2:<(3,4),(7,8)>$","cnt":[[294,557],[294,506],[584,506],[584,557]],"region":{"top_left_x":294,"top_left_y":505,"width":291,"height":52},"line":6,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9926886511966586,"confidence_rate":0.9995109057982067},{"text":"\nIf none of the documents meet the criteria, return None.","cnt":[[293,646],[293,600],[1231,600],[1231,646]],"region":{"top_left_x":292,"top_left_y":600,"width":939,"height":47},"line":7,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9990234375,"confidence_rate":0.9999839830848651},{"text":"\ni. \"I student\"","cnt":[[318,728],[318,682],[549,682],[549,728]],"region":{"top_left_x":318,"top_left_y":681,"width":231,"height":47},"line":8,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nii. \"student I\"","cnt":[[316,787],[316,741],[552,741],[552,787]],"region":{"top_left_x":315,"top_left_y":741,"width":237,"height":47},"line":9,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\niii. \"I / $2$ student\"","cnt":[[306,854],[306,805],[599,805],[599,854]],"region":{"top_left_x":306,"top_left_y":805,"width":294,"height":49},"line":10,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.6797385263716933,"confidence_rate":0.9863072152977328},{"text":"\niv. \"student / $2$ I\"","cnt":[[302,915],[302,863],[598,863],[598,915]],"region":{"top_left_x":302,"top_left_y":863,"width":296,"height":52},"line":11,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.8633328770151162,"confidence_rate":0.9945720094555742},{"text":"\nc. Let's say we want to find documents in which \"I\" and \"CS276\" are at most $3$ words apart, but","cnt":[[245,1002],[245,950],[1880,950],[1880,1002]],"region":{"top_left_x":245,"top_left_y":950,"width":1635,"height":52},"line":12,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.986340758157894,"confidence_rate":0.9998651719570275},{"text":" in the same sentence.","cnt":[[291,1042],[291,996],[652,996],[652,1042]],"region":{"top_left_x":291,"top_left_y":996,"width":361,"height":47},"line":13,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\ni. How would you modify the positional index to support queries that demand the terms to be","cnt":[[321,1122],[321,1073],[1880,1073],[1880,1122]],"region":{"top_left_x":321,"top_left_y":1073,"width":1559,"height":50},"line":14,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" in the same sentence? You can assume that the parsing step is able to identify the sentences","cnt":[[365,1175],[365,1123],[1880,1123],[1880,1175]],"region":{"top_left_x":364,"top_left_y":1123,"width":1516,"height":52},"line":15,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" in a document.","cnt":[[365,1216],[365,1170],[622,1170],[622,1216]],"region":{"top_left_x":364,"top_left_y":1169,"width":259,"height":47},"line":16,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nii. Write down an example of the modified postings list for the words \"I\" and \"CS276\".","cnt":[[312,1278],[312,1226],[1770,1226],[1770,1278]],"region":{"top_left_x":311,"top_left_y":1226,"width":1459,"height":52},"line":17,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9839493959443644,"confidence_rate":0.9998222047070381},{"text":"\n\n\\section{Problem $2$ - Dictionary Storage Compression (10 points)}","cnt":[[243,1418],[243,1353],[1611,1353],[1611,1418]],"region":{"top_left_x":242,"top_left_y":1352,"width":1370,"height":66},"line":18,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.7501180190593004,"confidence_rate":0.9953732409972519},{"text":"\n\nSuppose the vocabulary for your inverted index consists of the following $6$ terms:","cnt":[[243,1512],[243,1464],[1590,1464],[1590,1512]],"region":{"top_left_x":242,"top_left_y":1463,"width":1348,"height":50},"line":19,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nelite","cnt":[[339,1567],[339,1610],[423,1610],[423,1567]],"region":{"top_left_x":338,"top_left_y":1566,"width":85,"height":45},"line":20,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nelope","cnt":[[335,1660],[335,1614],[435,1614],[435,1660]],"region":{"top_left_x":334,"top_left_y":1614,"width":102,"height":47},"line":21,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nellipse","cnt":[[339,1704],[339,1657],[453,1657],[453,1704]],"region":{"top_left_x":338,"top_left_y":1657,"width":115,"height":47},"line":22,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\neloquent","cnt":[[339,1752],[339,1706],[491,1706],[491,1752]],"region":{"top_left_x":338,"top_left_y":1706,"width":153,"height":47},"line":23,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\neligible","cnt":[[337,1804],[337,1752],[465,1752],[465,1804]],"region":{"top_left_x":337,"top_left_y":1752,"width":128,"height":52},"line":24,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nelongate","cnt":[[337,1851],[337,1802],[487,1802],[487,1851]],"region":{"top_left_x":337,"top_left_y":1802,"width":150,"height":50},"line":25,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\na. Assume that the dictionary data structure used for this index stores the sorted list of terms","cnt":[[244,1947],[244,1901],[1881,1901],[1881,1947]],"region":{"top_left_x":243,"top_left_y":1901,"width":1639,"height":47},"line":26,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" using dictionary-as-a-string storage with front coding. Show the resulting storage of the above","cnt":[[293,1998],[293,1949],[1881,1949],[1881,1998]],"region":{"top_left_x":292,"top_left_y":1948,"width":1590,"height":50},"line":27,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" vocabulary of $6$ terms under two different block sizes:","cnt":[[291,2046],[291,1998],[1183,1998],[1183,2046]],"region":{"top_left_x":291,"top_left_y":1997,"width":893,"height":50},"line":28,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\ni. block size of $3$","cnt":[[321,2116],[321,2069],[611,2069],[611,2116]],"region":{"top_left_x":321,"top_left_y":2069,"width":291,"height":47},"line":29,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nii. block size of $6$","cnt":[[312,2175],[312,2129],[610,2129],[610,2175]],"region":{"top_left_x":311,"top_left_y":2129,"width":299,"height":47},"line":30,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\nUse the special symbols * and $\\diamond$ as used in the discussion on front coding in Chapter $5$.","cnt":[[294,2259],[294,2208],[1755,2208],[1755,2259]],"region":{"top_left_x":294,"top_left_y":2207,"width":1462,"height":53},"line":31,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.8100270503848606,"confidence_rate":0.9977611521335077},{"text":"\nb. Calculate the total dictionary storage (in bytes) required to store the vocabulary using:","cnt":[[244,2332],[244,2286],[1746,2286],[1746,2332]],"region":{"top_left_x":243,"top_left_y":2286,"width":1503,"height":47},"line":32,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\ni. Fixed width entries of $20$ bytes","cnt":[[318,2408],[318,2362],[885,2362],[885,2408]],"region":{"top_left_x":318,"top_left_y":2362,"width":567,"height":47},"line":33,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1}],"page_height":2750,"page_width":2125,"languages_detected":[]},{"image_id":"2025_04_27_83fd0f2a4bf8c26df6feg-3","page":3,"lines":[{"text":"\nii. Blocked storage with front coding and a block size of $3$ (i.e., Problem $2$ (a) (i) above)","cnt":[[312,308],[312,256],[1781,256],[1781,308]],"region":{"top_left_x":311,"top_left_y":256,"width":1470,"height":52},"line":1,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n\nBy what proportion do we reduce storage by using blocked storage with front coding and a block","cnt":[[293,384],[293,332],[1881,332],[1881,384]],"region":{"top_left_x":292,"top_left_y":332,"width":1590,"height":52},"line":2,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" size of $3$ ?","cnt":[[291,424],[291,378],[457,378],[457,424]],"region":{"top_left_x":291,"top_left_y":378,"width":166,"height":47},"line":3,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9975595474243164,"confidence_rate":0.9998714064091851},{"text":"\nAssume that:","cnt":[[291,484],[291,438],[522,438],[522,484]],"region":{"top_left_x":291,"top_left_y":437,"width":231,"height":47},"line":4,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n- The dictionary stores the term, the document frequency, and a pointer to the postings list","cnt":[[294,537],[294,488],[1820,488],[1820,537]],"region":{"top_left_x":294,"top_left_y":487,"width":1527,"height":50},"line":5,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9990234375,"confidence_rate":0.9999899274770393},{"text":"\n- The postings list pointer and the document frequency each occupy $4$ bytes","cnt":[[294,590],[294,538],[1560,538],[1560,590]],"region":{"top_left_x":294,"top_left_y":538,"width":1266,"height":52},"line":6,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n- Each character can be stored in $1$ byte","cnt":[[291,633],[291,581],[966,581],[966,633]],"region":{"top_left_x":291,"top_left_y":581,"width":676,"height":52},"line":7,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n- For blocked storage, the term pointer occupies $3$ bytes","cnt":[[291,682],[291,630],[1226,630],[1226,682]],"region":{"top_left_x":291,"top_left_y":630,"width":936,"height":52},"line":8,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9970715045928955,"confidence_rate":0.9999541761774452},{"text":"\nc. Suppose the above vocabulary was only a snippet of a larger vocabulary. What is the maximum","cnt":[[244,756],[244,707],[1881,707],[1881,756]],"region":{"top_left_x":243,"top_left_y":707,"width":1639,"height":50},"line":9,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" size of the vocabulary that can be resolved using $3$ byte term pointers? Assume that each","cnt":[[290,806],[290,755],[1884,755],[1884,806]],"region":{"top_left_x":290,"top_left_y":754,"width":1594,"height":53},"line":10,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9990234375,"confidence_rate":0.999989822555459},{"text":" vocabulary entry is $8$ characters long on average, and that blocked storage with front coding","cnt":[[294,855],[294,804],[1885,804],[1885,855]],"region":{"top_left_x":294,"top_left_y":803,"width":1592,"height":53},"line":11,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" compression method compressing the dictionary string by $20 \\%$. Show your calculations.","cnt":[[293,900],[293,851],[1751,851],[1751,900]],"region":{"top_left_x":292,"top_left_y":851,"width":1459,"height":49},"line":12,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9843823909759521,"confidence_rate":0.9998307578853929},{"text":"\n\n\\section{Problem $3$ - Postings Compression (8 points)}","cnt":[[244,1039],[244,977],[1334,977],[1334,1039]],"region":{"top_left_x":243,"top_left_y":977,"width":1091,"height":63},"line":13,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.6263684956356883,"confidence_rate":0.9908690712662275},{"text":"\n\nConsider a postings list:","cnt":[[245,1137],[245,1086],[655,1086],[655,1137]],"region":{"top_left_x":245,"top_left_y":1085,"width":410,"height":53},"line":14,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n$$<4,15,62,63,265,267,270,501>$$","cnt":[[764,1233],[764,1174],[1361,1174],[1361,1233]],"region":{"top_left_x":764,"top_left_y":1173,"width":597,"height":61},"line":15,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nwith the corresponding list of document gaps:","cnt":[[245,1327],[245,1275],[1012,1275],[1012,1327]],"region":{"top_left_x":245,"top_left_y":1275,"width":768,"height":52},"line":16,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\n$$<4,11,47,1,202,2,3,231>$$","cnt":[[809,1424],[809,1362],[1316,1362],[1316,1424]],"region":{"top_left_x":809,"top_left_y":1362,"width":507,"height":63},"line":17,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999787654211912},{"text":"\n\nAssume that the length of the postings list is stored separately, so the system knows when a postings","cnt":[[244,1512],[244,1464],[1881,1464],[1881,1512]],"region":{"top_left_x":243,"top_left_y":1463,"width":1639,"height":50},"line":18,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" list is complete. We are going to contrast the size and speed tradeoffs of using variable byte encoding","cnt":[[243,1559],[243,1510],[1882,1510],[1882,1559]],"region":{"top_left_x":242,"top_left_y":1509,"width":1641,"height":50},"line":19,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" with gamma encoding.","cnt":[[245,1609],[245,1557],[633,1557],[633,1609]],"region":{"top_left_x":245,"top_left_y":1557,"width":388,"height":52},"line":20,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999804641709025},{"text":"\na. Encode the document gaps using variable byte encoding.","cnt":[[245,1709],[245,1663],[1240,1663],[1240,1709]],"region":{"top_left_x":245,"top_left_y":1662,"width":996,"height":47},"line":21,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nb. Encode the document gaps using gamma encoding.","cnt":[[243,1793],[243,1741],[1145,1741],[1145,1793]],"region":{"top_left_x":242,"top_left_y":1741,"width":904,"height":52},"line":22,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nc. Assuming a slow ethernet connection (1 million bytes / second) to network attached storage,","cnt":[[245,1869],[245,1817],[1880,1817],[1880,1869]],"region":{"top_left_x":245,"top_left_y":1817,"width":1635,"height":52},"line":23,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9070040102815256,"confidence_rate":0.9990145423735365},{"text":" how long would it take to transfer just the given postings list stored in variable byte encoding","cnt":[[289,1916],[289,1868],[1880,1868],[1880,1916]],"region":{"top_left_x":288,"top_left_y":1867,"width":1592,"height":50},"line":24,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" (of document gaps) to memory? How about the gamma encoded list? (Answer in $\\mu \\mathrm{s}=10^{-6}$","cnt":[[293,1961],[293,1910],[1881,1910],[1881,1961]],"region":{"top_left_x":292,"top_left_y":1909,"width":1590,"height":53},"line":25,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9767316514626145,"confidence_rate":0.9997547870433742},{"text":" seconds.) Note: Do not pad the transmitted pad to the byte boundary.","cnt":[[291,2008],[291,1960],[1476,1960],[1476,2008]],"region":{"top_left_x":291,"top_left_y":1959,"width":1185,"height":50},"line":26,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nd. Now, assume that to decode a gamma encoded postings list into usable numbers takes a constant","cnt":[[243,2087],[243,2038],[1882,2038],[1882,2087]],"region":{"top_left_x":242,"top_left_y":2038,"width":1641,"height":50},"line":27,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" time of $n / 2 \\mu$ s where $n$ is the number of elements in the postings list. This adds an additional","cnt":[[293,2133],[293,2084],[1881,2084],[1881,2133]],"region":{"top_left_x":292,"top_left_y":2084,"width":1590,"height":50},"line":28,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.9833984375,"confidence_rate":0.999840575443913},{"text":" processing time to decode the posting list stored in gamma encoded format to documents gap","cnt":[[293,2183],[293,2132],[1881,2132],[1881,2183]],"region":{"top_left_x":292,"top_left_y":2131,"width":1590,"height":53},"line":29,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" integers. For the given postings list, what is the upper limit on how much processing time the","cnt":[[294,2228],[294,2179],[1885,2179],[1885,2228]],"region":{"top_left_x":294,"top_left_y":2179,"width":1592,"height":49},"line":30,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" decoding of variable byte encoding can take (in $\\mu$ s per list element), in order to be faster than","cnt":[[294,2273],[294,2227],[1885,2227],[1885,2273]],"region":{"top_left_x":294,"top_left_y":2226,"width":1592,"height":47},"line":31,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.765625,"confidence_rate":0.9974597767437118},{"text":" (or equally fast as) gamma encoding on an overall basis (transfer + decoding)?","cnt":[[293,2324],[293,2273],[1610,2273],[1610,2324]],"region":{"top_left_x":292,"top_left_y":2272,"width":1319,"height":53},"line":32,"column":0,"font_size":28,"is_printed":true,"is_handwritten":false,"confidence":0.7657487488754668,"confidence_rate":0.9967894880384621}],"page_height":2750,"page_width":2125,"languages_detected":[]},{"image_id":"2025_04_27_83fd0f2a4bf8c26df6feg-4","page":4,"lines":[{"text":"\n\n\\section{Problem $4$ - Tolerant Retrieval (7 points)}","cnt":[[247,308],[247,245],[1244,245],[1244,308]],"region":{"top_left_x":246,"top_left_y":245,"width":999,"height":63},"line":1,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.6123961894089689,"confidence_rate":0.9900422399423663},{"text":"\n\nConsider the following text:","cnt":[[245,404],[245,355],[709,355],[709,404]],"region":{"top_left_x":245,"top_left_y":355,"width":464,"height":49},"line":2,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\npeter piper picks papers","cnt":[[340,478],[340,432],[744,432],[744,478]],"region":{"top_left_x":340,"top_left_y":432,"width":405,"height":47},"line":3,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\na. How many character bigram dictionary entries and character bigram posting entries are generated","cnt":[[244,566],[244,518],[1881,518],[1881,566]],"region":{"top_left_x":243,"top_left_y":517,"width":1639,"height":50},"line":4,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" by indexing the bigrams in the terms in the text above? Use the special character $\\$$ to denote","cnt":[[293,608],[293,562],[1881,562],[1881,608]],"region":{"top_left_x":292,"top_left_y":562,"width":1590,"height":47},"line":5,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.99462890625,"confidence_rate":0.9999466789470377},{"text":" the beginning and end of terms.","cnt":[[293,657],[293,611],[829,611],[829,657]],"region":{"top_left_x":292,"top_left_y":611,"width":538,"height":47},"line":6,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nb. How would the wild-card query $\\mathbf{p}^{*}$ er be most efficiently expressed as an AND query using the","cnt":[[244,728],[244,682],[1881,682],[1881,728]],"region":{"top_left_x":243,"top_left_y":681,"width":1639,"height":47},"line":7,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.8066412718592346,"confidence_rate":0.9980833014478983},{"text":" bigram index over the text above?","cnt":[[291,777],[291,730],[869,730],[869,777]],"region":{"top_left_x":291,"top_left_y":730,"width":578,"height":47},"line":8,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nc. For the most efficient query in part 2, how many postings must be traversed? Briefly explain.","cnt":[[247,847],[247,801],[1878,801],[1878,847]],"region":{"top_left_x":246,"top_left_y":800,"width":1633,"height":48},"line":9,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.958984375,"confidence_rate":0.9995854275703437},{"text":" Note: the merge algorithm can only take $2$ lists at a time and has to traverse all postings in both","cnt":[[293,897],[293,848],[1881,848],[1881,897]],"region":{"top_left_x":292,"top_left_y":848,"width":1590,"height":50},"line":10,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" lists.","cnt":[[293,894],[293,938],[379,938],[379,894]],"region":{"top_left_x":292,"top_left_y":894,"width":88,"height":44},"line":11,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nd. How many permuterm dictionary entries are generated by indexing the words in the text above?","cnt":[[245,1014],[245,965],[1880,965],[1880,1014]],"region":{"top_left_x":245,"top_left_y":964,"width":1635,"height":50},"line":12,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\ne. How would the wild-card query $\\mathbf{p}^{*}$ er be expressed for lookup in the permuterm index?","cnt":[[243,1088],[243,1037],[1741,1037],[1741,1088]],"region":{"top_left_x":242,"top_left_y":1036,"width":1500,"height":53},"line":13,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.9888028870336711,"confidence_rate":0.9998917340097596},{"text":"\n\n\\section{Problem $5$ - Vector Space Model (10 points)}","cnt":[[244,1224],[244,1161],[1317,1161],[1317,1224]],"region":{"top_left_x":243,"top_left_y":1161,"width":1075,"height":63},"line":14,"column":0,"font_size":44,"is_printed":true,"is_handwritten":false,"confidence":0.78759765625,"confidence_rate":0.9952360257341638},{"text":"\n\nConsider the following documents:","cnt":[[243,1317],[243,1268],[820,1268],[820,1317]],"region":{"top_left_x":242,"top_left_y":1268,"width":578,"height":50},"line":15,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nA: to be not to be not","cnt":[[340,1392],[340,1351],[722,1351],[722,1392]],"region":{"top_left_x":340,"top_left_y":1351,"width":383,"height":41},"line":16,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.9990236759185791,"confidence_rate":0.9999638228397345},{"text":"\nB: it was not to be","cnt":[[340,1438],[340,1392],[668,1392],[668,1438]],"region":{"top_left_x":340,"top_left_y":1391,"width":329,"height":47},"line":17,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.99755859375,"confidence_rate":0.9998981555472577},{"text":"\na. Write down the term frequency of each term in each document.","cnt":[[245,1525],[245,1479],[1348,1479],[1348,1525]],"region":{"top_left_x":245,"top_left_y":1478,"width":1104,"height":47},"line":18,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nb. Write down the number of occurrences of each word bigram in each document (ignoring the","cnt":[[244,1599],[244,1550],[1881,1550],[1881,1599]],"region":{"top_left_x":243,"top_left_y":1550,"width":1639,"height":50},"line":19,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999948589553403},{"text":" begin-of-document and end-of-document tokens).","cnt":[[291,1644],[291,1598],[1107,1598],[1107,1644]],"region":{"top_left_x":291,"top_left_y":1597,"width":817,"height":47},"line":20,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nc. Consider a vector space where each dimension is a word bigram. For each document, write down","cnt":[[244,1714],[244,1668],[1881,1668],[1881,1714]],"region":{"top_left_x":243,"top_left_y":1668,"width":1639,"height":47},"line":21,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" the vector of normalized bigram term frequencies (without log scaling) using the format \\{bigram:","cnt":[[293,1765],[293,1716],[1881,1716],[1881,1765]],"region":{"top_left_x":292,"top_left_y":1715,"width":1590,"height":50},"line":22,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999951643631956},{"text":" value, bigram: value, ...\\}. Use Euclidean normalization.","cnt":[[291,1811],[291,1762],[1237,1762],[1237,1811]],"region":{"top_left_x":291,"top_left_y":1761,"width":947,"height":50},"line":23,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.7426467575896822,"confidence_rate":0.9951342570132734},{"text":"\nd. Now consider the tf-idf weighting:","cnt":[[244,1885],[244,1836],[862,1834],[862,1882]],"region":{"top_left_x":243,"top_left_y":1833,"width":620,"height":53},"line":24,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.99951171875,"confidence_rate":0.9999883714843234},{"text":"\n$$w_{t, d}=\\mathrm{tf}_{t, d} \\times \\log _{10}\\left(N / \\mathrm{df}_{t}\\right)$$","cnt":[[855,1953],[855,1891],[1313,1891],[1313,1953]],"region":{"top_left_x":855,"top_left_y":1890,"width":459,"height":63},"line":25,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.9980480668600649,"confidence_rate":0.9999674365194676},{"text":"\n\nFor each document, write down the vector of normalized tf -idf weights where each dimension is","cnt":[[293,2011],[293,1962],[1881,1962],[1881,2011]],"region":{"top_left_x":292,"top_left_y":1962,"width":1590,"height":50},"line":26,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.4659534125580766,"confidence_rate":0.992753340552888},{"text":" a word bigram. Perform Euclidean normalization after calculating the tf -idf weight vector $w_{t, d}$.","cnt":[[293,2056],[293,2010],[1881,2010],[1881,2056]],"region":{"top_left_x":292,"top_left_y":2009,"width":1590,"height":47},"line":27,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.8628935808005167,"confidence_rate":0.9987510864019011},{"text":" As in c, use the format \\{bigram: value, bigram: value, ...\\}. Assume that the corpus only has","cnt":[[293,2106],[293,2057],[1881,2057],[1881,2106]],"region":{"top_left_x":292,"top_left_y":2057,"width":1590,"height":50},"line":28,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.7414065012612726,"confidence_rate":0.996920152291372},{"text":" the two documents listed above.","cnt":[[294,2149],[294,2106],[833,2106],[833,2149]],"region":{"top_left_x":294,"top_left_y":2106,"width":540,"height":44},"line":29,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\ne. For each of the weighting schemes in parts c and d, compute the cosine similarity between","cnt":[[244,2223],[244,2171],[1881,2174],[1881,2225]],"region":{"top_left_x":243,"top_left_y":2171,"width":1639,"height":55},"line":30,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.5135773848742247,"confidence_rate":0.9931539073602511},{"text":" documents $A$ and $B$. Briefly explain any insights you draw from your answers.","cnt":[[294,2271],[294,2223],[1603,2223],[1603,2271]],"region":{"top_left_x":294,"top_left_y":2222,"width":1310,"height":50},"line":31,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":"\nf. Let's say you now have the opportunity to add one document to the corpus. Design a document J'","cnt":[[249,2346],[249,2294],[1881,2294],[1881,2346]],"region":{"top_left_x":249,"top_left_y":2294,"width":1633,"height":52},"line":32,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":1,"confidence_rate":1},{"text":" to add to the corpus such that the cosine similarity of A and B under part d remains unchanged.","cnt":[[291,2391],[291,2342],[1877,2342],[1877,2391]],"region":{"top_left_x":291,"top_left_y":2341,"width":1586,"height":50},"line":33,"column":0,"font_size":35,"is_printed":true,"is_handwritten":false,"confidence":0.5531172208593489,"confidence_rate":0.9949940520079216}],"page_height":2750,"page_width":2125,"languages_detected":[]}]}